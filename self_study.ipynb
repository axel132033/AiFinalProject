{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before Running the notebook you will need to install the following \n",
    "# Remove the '#' from the code \n",
    "\n",
    "\n",
    "# pip install numpy==1.24\n",
    "# pip install beautifulsoup4==4.11.1\n",
    "# pip install selenium==4.9.0\n",
    "# pip install requests==2.28.1\n",
    "# pip install webdriver-manager==3.8.5\n",
    "# pip install torch==1.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import typing as t\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler\n",
    "\n",
    "- Initiate Crawler that Take all song from daddy yankee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crawler\n",
    "\n",
    "- Scrapper class that crawls and extract all information from the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "\n",
    "    \"\"\" Music Crawler\n",
    "    \n",
    "    #### Arguments:\n",
    "        - url : main url\n",
    "        - save_at : path where information will be saved\"\"\"\n",
    "\n",
    "    def __init__(self, url: str) -> None:\n",
    "        self.url = url\n",
    "        self.options = self._config_options()\n",
    "        self.all_songs = []\n",
    "        self.lyrics_string = \"\"\n",
    "\n",
    "    def _init_webdriver(self) -> webdriver:\n",
    "        \"\"\" Initiate WebDriver Edge\"\"\"\n",
    "        driver = webdriver.Edge(\n",
    "            service=Service(EdgeChromiumDriverManager().install()),\n",
    "            options=self.options,\n",
    "        )\n",
    "        return driver\n",
    "    \n",
    "    def _config_options(self) -> dict:\n",
    "        \"\"\" Set up driver options\"\"\"\n",
    "        self.options = Options()\n",
    "        self.options.add_experimental_option(\"detach\", True)\n",
    "        return self.options\n",
    "    \n",
    "\n",
    "    def _extract_all_songs(self, url : str) -> t.Union[str, bool]:\n",
    "        # Inititate browser\n",
    "        brw = self._init_webdriver()\n",
    "        brw.get(url)\n",
    "\n",
    "        # Take all songs \n",
    "        list_of_songs = brw.find_elements(By.XPATH, \"//ul[@class='songList-table-content js-song-list']\")\n",
    "        for song in list_of_songs:\n",
    "            given_song = song.find_elements(By.TAG_NAME, \"li\")\n",
    "            for link in given_song:\n",
    "                self.all_songs.append(link.get_attribute(\"data-shareurl\"))\n",
    "\n",
    "        # Close Browser\n",
    "        brw.close()\n",
    "        \n",
    "        # Check we have links\n",
    "        if self.all_songs:\n",
    "            return self.all_songs\n",
    "        return False\n",
    "    \n",
    "    def _init_beautifulsoup (self, list_of_song: t.List[str]) -> str:\n",
    "        \"\"\" Use Beautiful Soup to Get Lyrics\"\"\"\n",
    "\n",
    "        # Loop Thorugh Each song\n",
    "        for song_ in list_of_song:\n",
    "            # Iniate BSoup\n",
    "            web = requests.get(song_)\n",
    "            if web.status_code == 200:\n",
    "                bs = BeautifulSoup(web.content, \"html.parser\")\n",
    "\n",
    "                # Extract Song\n",
    "                for lyric in bs.find_all(\"div\", attrs={'class':'lyric-original'}):\n",
    "                    for line in lyric.find_all(\"p\"):\n",
    "                        for tag in line.contents:\n",
    "                            if tag.text == \"\":\n",
    "                                continue\n",
    "                            else:\n",
    "                                self.lyrics_string += f\"{tag.text.lower().strip()} \" \n",
    "                    self.lyrics_string += \"\\n\" \n",
    "\n",
    "        return self.lyrics_string\n",
    "\n",
    "    def StartCrawling(self):\n",
    "        \"\"\" Start Crawling\"\"\"\n",
    "\n",
    "        # List of Songs\n",
    "        list_of_song = self._extract_all_songs(self.url)\n",
    "        if list_of_song:\n",
    "            self.lyrics_string = self._init_beautifulsoup(list_of_song)\n",
    "        else:\n",
    "            raise ValueError (\"Error at Crawling all Lyrics\")\n",
    "\n",
    "\n",
    "        # Save files\n",
    "        file_save_at = \"daddy_yankee_song.txt\"\n",
    "        with open (file_save_at, \"w\", encoding=\"utf-8\") as FiletoSave:\n",
    "            FiletoSave.write(self.lyrics_string)\n",
    "\n",
    "        print(\"-------- Done ---------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Done ---------\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.letras.com/daddy-yankee/\"\n",
    "test = Crawler(url).StartCrawling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization \n",
    "\n",
    "- There are two types of tokenization \n",
    "1. letter by letter : which aims to predit the following word for example : hello how are you | space | r |  ?\n",
    "2. Word by word: hello how -> are you | you doing ?\n",
    "\n",
    "but of course the example are related to the song "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Inncesary Tokens to speed up the training process\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append specially tokens into list \n",
    "EOS, UNK, PAD = \"<EOS>\",\"<UNK>\",\"<PAD>\"\n",
    "\n",
    "# All Spanish Character\n",
    "all_letters_in_spanish = \"\"\"\n",
    "áéèíóúüabcdefghijklmnñopqrstuvwxyz0123456789 -,;.[¡¿!?]:“'’’’/\\|_@#$%ˆ&*˜‘+-=()[]{}\"'ÁÉÍÓÚABCDEFGHIJKLMNÑOPQRSTUVWXYZ\n",
    "\"\"\" \n",
    "\n",
    "# Define Chars and tranforme them into list\n",
    "chars = list(sorted(set(all_letters_in_spanish)))\n",
    "for spc_token in [EOS, UNK, PAD]:\n",
    "    # Speciall Character for Sequences Creation\n",
    "    if spc_token == \"<PAD>\":\n",
    "        chars.insert(0, spc_token)\n",
    "    chars.append(spc_token)\n",
    "\n",
    "# Creating Unique idx for each character\n",
    "encoder = {k: v for v, k in enumerate(chars)}\n",
    "decoder =  np.array(chars)\n",
    "\n",
    "# char_index\n",
    "def char_idx(c : str):\n",
    "    if c in chars:\n",
    "        return encoder[c]\n",
    "    return encoder[UNK]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Settings : all Settings are define in here\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRING_ENCODING = \"utf-8\"\n",
    "MAX_LEN = 75  # Adjust to the maxium number of word per music\n",
    "PATH = \"./daddy_yankee_song.txt\" # Path to the file containing the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Processing letter by letter\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Data file loaded ****\n"
     ]
    }
   ],
   "source": [
    "Letterbyletter = list()\n",
    "with open(PATH, \"r\", encoding=STRING_ENCODING) as File2Process:\n",
    "    songs_ = File2Process.read().split(\"\\n\")\n",
    "    # Looping Line By Line\n",
    "    for line in songs_:\n",
    "        single_line = line.strip()\n",
    "\n",
    "        # Convert and pass into Decoder\n",
    "        char_transformed = [char_idx(c) for c in single_line[:-1]]\n",
    "        if len(char_transformed) >= MAX_LEN:\n",
    "            char_transformed = char_transformed[0:MAX_LEN-1]\n",
    "            char_transformed.append(encoder[EOS])\n",
    "        else:\n",
    "            char_transformed.append(encoder[EOS])\n",
    "            remain = MAX_LEN - len(char_transformed)\n",
    "            if remain > 0:\n",
    "                char_transformed.extend([encoder[PAD]] * remain)\n",
    "        Letterbyletter.append(char_transformed)\n",
    "\n",
    "print(\"**** Data file loaded ****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65, 77, 75, 71, 76]\n",
      "c o m i n g   f r o m   t h e   s h a d o w   o f   t h e   i s l a n d !   w e   a r e   w i n c h e s t e r   y a n k e e   a n d   n a s   e s c <EOS> \n"
     ]
    }
   ],
   "source": [
    "letter_string = \"\"\n",
    "print(Letterbyletter[0][:5])\n",
    "for i in Letterbyletter[0]:\n",
    "    letter_string += f\"{decoder[i]} \"\n",
    "\n",
    "print(letter_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Processing Word by Word\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_by_word_processing (lyric:t.List[str]):\n",
    "    \"\"\" Tokenize word by word\"\"\"\n",
    "\n",
    "    # Concate all songs\n",
    "    all_songs = \" \".join(song.strip() for song in lyric)\n",
    "\n",
    "    # Cleaning Corpus\n",
    "    corpus = set(all_songs.split(\" \"))\n",
    "\n",
    "    # creating decoder \n",
    "    word_decoder = {v:num for num, v in enumerate(list(corpus) + [EOS, UNK, PAD], 0)}\n",
    "\n",
    "    return word_decoder\n",
    "\n",
    "def word_to_idx(word:str, tokenizer:dict):\n",
    "    \"\"\" map words\"\"\"\n",
    "    if tokenizer.get(word, UNK):\n",
    "        return tokenizer[word]\n",
    "    return tokenizer[UNK]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordProcess = list()\n",
    "with open(PATH, \"r\", encoding=STRING_ENCODING) as File2Process:\n",
    "    # Creating Corpus\n",
    "    songs_ = File2Process.read().split(\"\\n\")\n",
    "\n",
    "    # Create decoder\n",
    "    WordEncoder = word_by_word_processing(songs_)\n",
    "    WordDecoder = {v:k for k, v in WordEncoder.items()}\n",
    "\n",
    "    # Looping Line By Line\n",
    "    for line in songs_:\n",
    "        single_line = line.strip()\n",
    "\n",
    "        # Convert and pass into Decoder\n",
    "        char_transformed = [word_to_idx(c,WordEncoder) for c in single_line.split(\" \")[:-1]]\n",
    "        if len(char_transformed) >= MAX_LEN:\n",
    "            char_transformed = char_transformed[0:MAX_LEN-1]\n",
    "            char_transformed.append(WordEncoder[EOS])\n",
    "        else:\n",
    "            char_transformed.append(WordEncoder[EOS])\n",
    "            remain = MAX_LEN - len(char_transformed)\n",
    "            if remain > 0:\n",
    "                char_transformed.extend([WordEncoder[PAD]] * remain)\n",
    "        WordProcess.append(char_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6285, 3390, 5095, 10289, 10000, 5095, 10580, 15298, 7850, 8150, 5302, 14966, 15585, 10127, 7332, 2722, 15731, 198, 770, 2690, 16761, 18633, 18782, 9621, 18858, 9204, 12268, 1474, 1418, 13782, 14418, 2005, 7968, 18633, 13782, 9076, 3163, 9204, 16243, 19869, 9246, 8150, 18984, 962, 5481, 905, 2811, 8259, 9506, 9202, 2313, 5907, 16145, 12899, 9204, 1476, 1474, 6226, 20521, 14367, 9626, 18569, 13990, 14367, 2324, 1474, 14367, 8739, 4545, 9506, 8208, 4966, 340, 11424, 20671]\n",
      "coming from the shadow of the island! we are winchester yankee and nas escobar! pablo, what's up, pa'? treinta-treinta, 70mm metras es letal, violenta alimenta el mental de toda mi gente completa fundamenta es mi letra, representa el instrumental rápida lenta, winchester inventa en los noventa líricas respuestas para preguntas que no contestan como el misterio de cuando suenen la trompeta 666, será la marca de la bestia directo para tu frente o a <EOS> \n"
     ]
    }
   ],
   "source": [
    "print(WordProcess[0])\n",
    "example = \"\"\n",
    "for i in WordProcess[0]:\n",
    "    example += f\"{WordDecoder[i]} \"\n",
    "\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last Transformation - Numpy into `Torch Tensors` \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classes and Methods for the Architecture - Letter by Letter\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to numpy\n",
    "LetterNumpyData = torch.tensor(Letterbyletter)\n",
    "\n",
    "# Training and Testing\n",
    "np_data_in = LetterNumpyData[:, :-1]\n",
    "np_data_out = LetterNumpyData[:, 1:]\n",
    "\n",
    "# Creating TensorDataset\n",
    "Letterdataset = TensorDataset(np_data_in, np_data_out)\n",
    "\n",
    "# Creating DataLoader\n",
    "BATCH_SIZE = 32\n",
    "Letterdataloader = DataLoader(Letterdataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classes and Methods for the Architecture - word by word\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to numpy\n",
    "WordNumpyData = torch.tensor(WordProcess)\n",
    "\n",
    "# Training and Testing\n",
    "np_data_in = WordNumpyData[:, :-1]\n",
    "np_data_out = WordNumpyData[:, 1:]\n",
    "\n",
    "# Creating TensorDataset\n",
    "Worddataset = TensorDataset(np_data_in, np_data_out)\n",
    "\n",
    "# Creating DataLoader\n",
    "BATCH_SIZE = 32\n",
    "Worddataloader = DataLoader(Worddataset, batch_size=BATCH_SIZE, shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module Creation \n",
    "\n",
    "- Classes and Methods for the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class MyGenerator(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size : int,\n",
    "                 embedding_dim : int,\n",
    "                 hidden_size : int):\n",
    "        \"\"\" \n",
    "        ### Arguments:\n",
    "            - vocab_size: total number of vocabulary\n",
    "            - embedding_dim: embedding dimension for word transformation\n",
    "            - hidden_size: number of hidden layers\n",
    "        \"\"\"\n",
    "        super(MyGenerator, self).__init__()\n",
    "\n",
    "        # Set and Initiate Layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.gru1 = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.gru2 = nn.LSTM(hidden_size, 560, batch_first=True)\n",
    "        self.fc = nn.Linear(560, vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Training Model\n",
    "        ####\n",
    "            Arguments:\n",
    "                - x: Input\n",
    "        #### \n",
    "            Returns:\n",
    "                - prediction : torch.tensor\n",
    "        \"\"\"\n",
    "\n",
    "        # Fitting embeddings\n",
    "        embedding_ = self.embedding(x)\n",
    "        before_layer_1 = self.dropout(embedding_)\n",
    "\n",
    "        # First Gru layer\n",
    "        gru_1_output, _ = self.gru1(before_layer_1)\n",
    "        before_layer_2 = self.dropout(gru_1_output)\n",
    "\n",
    "        # Second Gru layer\n",
    "        gru_2_output, _ = self.gru2(before_layer_2)\n",
    "        before_final_dense = self.dropout(gru_2_output)\n",
    "\n",
    "        # Final Prediction\n",
    "        prediction = self.fc(before_final_dense)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "\n",
    "def generate_text(model, decoder , start_string, temperature=0.95, num_generate=5):\n",
    "    model.eval()\n",
    "\n",
    "    # Convert string to numbers\n",
    "    input_eval = torch.tensor([encoder[s] for s in start_string], dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    # Text generation\n",
    "    text_generated = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_generate):\n",
    "            predictions = model(input_eval)\n",
    "            predictions = F.softmax(predictions / temperature, dim=-1)\n",
    "            predicted_id = torch.multinomial(predictions[:, -1, :], 1).item()\n",
    "\n",
    "            input_eval = torch.tensor([[predicted_id]], dtype=torch.long)\n",
    "            text_generated.append(decoder[predicted_id])\n",
    "\n",
    "    return start_string + ' '.join(text_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Settings for Model and Training\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Settings for Letter by letter\n",
    "VOC_SIZE = len(chars) # all unique characters\n",
    "\n",
    "### Setting for word by word \n",
    "VOC_SIZE_WORD = len(WordEncoder.keys())\n",
    "\n",
    "### General Setting\n",
    "EMB_DIM = 32 # Emb dimension\n",
    "HIDDEN_SIZE = 1024 # Gru Unit in internal layer\n",
    "EPOCHS = 20 # Number of Training Epochs\n",
    "LR = 0.001  # Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "\n",
    "### Training\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Training : letter by letter\n",
    "\n",
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 3.295713186264038\n",
      "Epoch [2/20], Loss: 3.105656385421753\n",
      "Epoch [3/20], Loss: 3.086557626724243\n",
      "Epoch [4/20], Loss: 2.960405111312866\n",
      "Epoch [5/20], Loss: 3.0316011905670166\n",
      "Epoch [6/20], Loss: 2.8633739948272705\n",
      "Epoch [7/20], Loss: 2.8617310523986816\n",
      "Epoch [8/20], Loss: 2.9382972717285156\n",
      "Epoch [9/20], Loss: 2.4017539024353027\n",
      "Epoch [10/20], Loss: 2.606152296066284\n",
      "Epoch [11/20], Loss: 2.546105146408081\n",
      "Epoch [12/20], Loss: 2.608675718307495\n",
      "Epoch [13/20], Loss: 2.5434482097625732\n",
      "Epoch [14/20], Loss: 2.6174988746643066\n",
      "Epoch [15/20], Loss: 2.4513144493103027\n",
      "Epoch [16/20], Loss: 1.8473807573318481\n",
      "Epoch [17/20], Loss: 2.334038734436035\n",
      "Epoch [18/20], Loss: 2.1861321926116943\n",
      "Epoch [19/20], Loss: 2.086322784423828\n",
      "Epoch [20/20], Loss: 2.0540640354156494\n",
      "***** End Training *****\n"
     ]
    }
   ],
   "source": [
    "# Initiate Model\n",
    "modelletter = MyGenerator(\n",
    "    vocab_size=VOC_SIZE,\n",
    "    embedding_dim= EMB_DIM,\n",
    "    hidden_size=HIDDEN_SIZE\n",
    ")\n",
    "\n",
    "# Loss and Optimization \n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(modelletter.parameters(), lr=LR)\n",
    "\n",
    "# Training \n",
    "for epoch in range(EPOCHS):\n",
    "    # taking input, output from transformed data\n",
    "    for inputs, targets in Letterdataloader:\n",
    "        optimizer.zero_grad() # Zero gradients\n",
    "        outputs = modelletter(inputs.long())\n",
    "\n",
    "        # define loss\n",
    "        loss = criteria(\n",
    "            outputs.transpose(1, 2),\n",
    "            targets.long())\n",
    "        \n",
    "        # back-propagination\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimization\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item()}')\n",
    "\n",
    "print(\"***** End Training *****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training : word by word\n",
    "\n",
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 8.0880765914917\n",
      "Epoch [2/20], Loss: 7.641803741455078\n",
      "Epoch [3/20], Loss: 7.34825325012207\n",
      "Epoch [4/20], Loss: 7.369215488433838\n",
      "Epoch [5/20], Loss: 7.200517654418945\n",
      "Epoch [6/20], Loss: 7.217112064361572\n",
      "Epoch [7/20], Loss: 7.193727016448975\n",
      "Epoch [8/20], Loss: 7.315526008605957\n",
      "Epoch [9/20], Loss: 7.240156650543213\n",
      "Epoch [10/20], Loss: 7.083902835845947\n",
      "Epoch [11/20], Loss: 7.112813472747803\n",
      "Epoch [12/20], Loss: 7.34207010269165\n",
      "Epoch [13/20], Loss: 7.223349094390869\n",
      "Epoch [14/20], Loss: 7.211241245269775\n",
      "Epoch [15/20], Loss: 7.1130757331848145\n",
      "Epoch [16/20], Loss: 6.990081787109375\n",
      "Epoch [17/20], Loss: 7.140476703643799\n",
      "Epoch [18/20], Loss: 7.065708637237549\n",
      "Epoch [19/20], Loss: 7.0598955154418945\n",
      "Epoch [20/20], Loss: 7.0329999923706055\n",
      "***** End Training *****\n"
     ]
    }
   ],
   "source": [
    "# Initiate Model\n",
    "modelword = MyGenerator(\n",
    "    vocab_size=VOC_SIZE_WORD,\n",
    "    embedding_dim= EMB_DIM,\n",
    "    hidden_size=HIDDEN_SIZE\n",
    ")\n",
    "\n",
    "# Loss and Optimization \n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(modelword.parameters(), lr=LR)\n",
    "\n",
    "# Training \n",
    "for epoch in range(EPOCHS):\n",
    "    # taking input, output from transformed data\n",
    "    for inputs_, targets_ in Worddataloader:\n",
    "        optimizer.zero_grad() # Zero gradients\n",
    "        outputs_ = modelword(inputs_.long())\n",
    "\n",
    "        # define loss\n",
    "        loss = criteria(\n",
    "            outputs_.transpose(1, 2),\n",
    "            targets_.long())\n",
    "        \n",
    "        # back-propagination\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimization\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item()}')\n",
    "\n",
    "print(\"***** End Training *****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "### Comparing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- letter by letter prediction \n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained on Letter by Letter: \n",
      " Dale Mas gasolina e s o h e n o o h i n u n o h i e n o t u h e n o h a r ú u o h ú i s u e l l o h e s i s i h e l e s o o h o h a o c o h e l o h e l e l e l e s e   ¡ s i a m u a a h e u n o h o h e l e n o h o h é\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "start_sequence = \"Dale Mas gasolina \"\n",
    "generated_text = generate_text(modelletter, decoder, start_sequence, temperature=0.7, num_generate=100)\n",
    "print(f\"Trained on Letter by Letter: \\n {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word by Word prediction \n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained on Letter by Letter: \n",
      " a ella le gustaque aunque tu discoteca c'mon miami hielo, bailalo... (what?!) si te ziggy, menes menor están vip... yankee ti-tírale) madre sabes ey, le los el cuestión me se te me oh, yankee, hey! siente hey! daddy\n"
     ]
    }
   ],
   "source": [
    "start_sequence = \"a ella le gusta\"\n",
    "generated_text = generate_text(modelword, WordDecoder, start_sequence, temperature=0.5, num_generate=35)\n",
    "print(f\"Trained on Letter by Letter: \\n {generated_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
